{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ZPxoeLGjplW"
   },
   "source": [
    "# Introduction to Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0iK5DyBvjplj"
   },
   "source": [
    "## Learning Goals \n",
    "The goal of the Business Analytics exercise is to **teach all steps necessary to solve a predictive data analytics task** using machine learning/neural networks. As the basis for any machine learning is data, in this exercise, we will look into how to load and work with tabular data. \n",
    "\n",
    "For this, we use a CSV (comma-separated values) file that contains information about books from [Goodreads](https://www.goodreads.com). In this exercise, we will clean this data and further parse it.  We will do some exploratory data analysis to answer questions about these books and popular genres. \n",
    "\n",
    "After this introductory exercise for the Python package Pandas, you will feel more comfortable:\n",
    "\n",
    "- Loading and working with tabular data. \n",
    "- Getting a first overview over the data in a Pandas Dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lRtIHa5xpPGr"
   },
   "source": [
    "## Importing modules\n",
    "All notebooks should begin with code that imports *packages*, collections of built-in, commonly-used Python functions.  Below we import the Matplotlib package, a library for plotting images, lines, graphs, ...  Future exercises will require additional modules, which we'll import with the same syntax.\n",
    "\n",
    "`import MODULE_NAME as MODULE_NICKNAME` \n",
    "\n",
    "In the following, we can import the package named Pandas and we give it a nickname ```pd```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wdhBfZFzpFyy"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UL4yjEgCjpll"
   },
   "source": [
    "## Loading and Cleaning with Pandas \n",
    "Pandas is a Python package that allows you to work with dataframes. Dataframes are two-dimensional arrays and look just like Excel-sheets. In fact, Pandas provides functionality to read Excel files as well. However, the files in our exercises come in so-called CSV (comma-separated values) format. You can simply open them in a text editor and have a look. \n",
    "\n",
    "A CSV file ```file.csv``` can be read into a variable ```df``` (denoting a Pandas dataframe) using \n",
    "\n",
    "```\n",
    "df = pd.read_csv(file.csv)\n",
    "```. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IskuqVxCp2S7"
   },
   "source": [
    "### Exercise\n",
    "\n",
    "1. Download or checkout the csv file `goodreads.csv` from github. Upload it to Google colab, if you are using this tool; For this, you can click on \"Files\" or \"Dateien\" on the left side and then click on \"Upload\". The file should then be available under \"data/goodreads.csv\", relative to the Jpupyter notebook.\n",
    "2. Load the file using the function ```pd.read_csv``` and store it into the variable ```df```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TJSsWPW6TPWc"
   },
   "outputs": [],
   "source": [
    "####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R4z-oQ_Speer"
   },
   "source": [
    "Here is a description of the columns (in order) present in this csv file:\n",
    "\n",
    "```\n",
    "rating: the average rating on a 1-5 scale achieved by the book\n",
    "review_count: the number of Goodreads users who reviewed this book\n",
    "isbn: the ISBN code for the book\n",
    "booktype: an internal Goodreads identifier for the book\n",
    "author_url: the Goodreads (relative) URL for the author of the book\n",
    "year: the year the book was published\n",
    "genre_urls: a string with '|' separated relative URLS of Goodreads genre pages\n",
    "dir: a directory identifier internal to the scraping code\n",
    "rating_count: the number of ratings for this book (this is different from the number of reviews)\n",
    "name: the name of the book\n",
    "```   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EgiHKkqequ0j"
   },
   "source": [
    "Let us see what issues we find with the data and resolve them. For this, you can simple type in ```print(df)```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JeVTriUCTWn5"
   },
   "outputs": [],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QDDFk9rZjplr"
   },
   "source": [
    "Oh dear. That does not quite seem to be right. We are missing the column names. We need to add these in! But what are they?\n",
    "\n",
    "Here is a list of them in order:\n",
    "\n",
    "`['rating', 'review_count', 'isbn', 'booktype','author_url', 'year', 'genre_urls', 'dir','rating_count', 'name']`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r8mf6GhAjplr"
   },
   "source": [
    "### Exercise \n",
    "\n",
    "1. Use the list of column names to properly read in the CSV file (have a look at the documentation for pd.read_csv to see how this is done - https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jN39mRx0TZvE"
   },
   "outputs": [],
   "source": [
    "####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D66yo7sXEtNz"
   },
   "source": [
    "## Getting a First Impression\n",
    "\n",
    "What I like doing first is having a look at the shape of the dataframe, i.e., the number of observations (rows) and the number of variables (columns). You can use the functions\n",
    "\n",
    "- ```df.shape```\n",
    "- ```len(df)```\n",
    "- ```len(df.columns)```\n",
    "\n",
    "for that. This also tells you, how memory intense it is to work with the dataframe. If there are millions of rows, you have to be way more careful to pick efficient functions when altering the dataframe.\n",
    "\n",
    "Afterwards, you can have a look at the first and last rows of the dataframe. These can be obtained by \n",
    "\n",
    "- ```df.head()```\n",
    "- ```df.tail()```\n",
    "\n",
    "### Exercise\n",
    "1. Get an impression about the shape of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2fSL1FmyTheB"
   },
   "outputs": [],
   "source": [
    "####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4kR49HUGjpls"
   },
   "source": [
    "## Cleaning: Examing the dataframe - quick checks\n",
    "\n",
    "First, we should have a look at the data types of each column. This usually already gives a good impression about what kind of values there appear in a column. You can use ```df.dtypes``` for that.\n",
    "\n",
    "### Exercise\n",
    "1. Have a look at the data types and discuss them. Do they make sense? We will later fix some of the data types.\n",
    "\n",
    "- float is a floating point number (e.g., 1.342)\n",
    "- object is used in Pandas for storing strings (e.g., \"Steven King\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sjbrUIneTlfn"
   },
   "outputs": [],
   "source": [
    "####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p1RrVX9DZQc6"
   },
   "source": [
    "## Data Selection\n",
    "\n",
    "Frequently, you want to select a single column and have some operation on that column. You can access a single columns with ```df[column_name]```. Let's have a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZbEp46wKIPxh"
   },
   "outputs": [],
   "source": [
    "df['name'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HCsn0PTRgbhn"
   },
   "outputs": [],
   "source": [
    "# select 2 columns\n",
    "# Watch out: if you want multiple columns, you have to pass it as a list\n",
    "df[['isbn', 'name']].head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v0omUnjEg2dU"
   },
   "outputs": [],
   "source": [
    "col_a = 'isbn'\n",
    "col_b = 'name'\n",
    "\n",
    "df[[col_a, col_b]].tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XYdWRpyzc3oG"
   },
   "source": [
    "## Indexing\n",
    "Indexing means that you want to access one or multiple rows. There are two different ways to do that.\n",
    "\n",
    "- First, you can use the **integer index of the rows** (starting with 0) using **`df.iloc`**. Thereby, the ```n-th``` row is accessed by ```df.iloc[n-1]```. This is quite similar to indexing in lists, which we covered in the last exercise. With `.iloc`, the elements are counted from the top down.\n",
    "- A second approach is to use the **row labels (i.e., index) using `.loc`**. You have probably seen above that there are these bold numbers at the very left of the output of the dataframe. These are called the index in the world of Pandas. You can select the index ```i``` by calling ```df.loc[i]```. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the first row -> notice: iloc uses the row numbers but not the value of the index\n",
    "df.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the moment, the integer index (`.iloc`) and the index (`.loc`) lead to the same result, but this will change later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pppKHkpQh0jM"
   },
   "outputs": [],
   "source": [
    "# get the last 5 rows\n",
    "df.iloc[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pE8ZmfYWyqCK"
   },
   "source": [
    "### Exercise\n",
    "1. Do you remember the negativ index? If not, you can discuss it briefly again in your group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dZUQHlCoiI6h"
   },
   "outputs": [],
   "source": [
    "# loc works with the actual values of the index, not with the row number. We will see that this makes a difference later\n",
    "df.loc[[5995, 5999]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WQRKvNp6hbTG"
   },
   "source": [
    "## Choose rows by condition\n",
    "\n",
    "The fun usually starts, when cutting rows out of the dataframe and have a closer look at them. Let's have a look at how to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7SQIsRq_jwlE"
   },
   "outputs": [],
   "source": [
    "# check whether book was published after 2010\n",
    "df['year'] > 2010"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bniQrNwYJqLJ"
   },
   "source": [
    "You see a Boolean value for each row if the entry in the column ```year``` is greater than 2010. This output is not too informative itself. However, you can store the output and then find out statistics, such as the sum or the mean.\n",
    "\n",
    "### Exercise\n",
    "\n",
    "1. Store the above output in a variable ```x```.\n",
    "2. Determine the data type of ```x```.\n",
    "3. Search online, how to get the sum and the mean of ```x```. Does the sum and mean make sense on Boolean values? Discuss in the group what these numbers represent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1sIacS2ITyK-"
   },
   "outputs": [],
   "source": [
    "####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P7zRA8vlKwDA"
   },
   "source": [
    "\n",
    "With the previous exercise, we already have a feeling about what kind of books there are in the dataframe in terms of year published. But now, we want to have all the information about these books. We can do this as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W0iHGt7Oj1G9"
   },
   "outputs": [],
   "source": [
    "# get all entries where the book was published after 2010\n",
    "df_after_2010 = df[df['year'] > 2010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GYKkoSgQkEOP"
   },
   "outputs": [],
   "source": [
    "df_after_2010.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M0_pjOVIzO0b"
   },
   "source": [
    "Note, that the bolt index at the very left now does not simply count the rows. The reason for this is, that the dataframe ```df_after_2010``` is only a view on the original dataframe ```df``` (i.e., the values in the dataframe are not copied in the memory)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dzLJdDcGLD7A"
   },
   "source": [
    "### Exercise\n",
    "\n",
    "1. Recall the difference between ```df.loc``` and ```df.iloc``` and select the 5-th row and then the row with index 299 of the dataframe ```df_after_2010```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ls7eUOPoYn-G"
   },
   "outputs": [],
   "source": [
    "####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WXdxcUHwkGY4"
   },
   "outputs": [],
   "source": [
    "# Lets see how many books were published after 2010\n",
    "df_after_2010.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_-9scY5ckJAg"
   },
   "outputs": [],
   "source": [
    "# Lets get the books that were published after 2010 and which have a rating of 5.0\n",
    "df_after_2010_rating_equal_5 = df[(df['year'] > 2010) & (df['rating'] == 5.0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DAkpFSnbLep0"
   },
   "source": [
    "Now it's becoming more interesting. You can combine the selection criteria. You can combine them as follows:\n",
    "\n",
    "- ```x & y``` means ```x AND y```. So you tell that you want to have both criterias being True.\n",
    "- ```x | y``` means ```x OR y```. So you tell that you want at least one of the criterias being True (or both).\n",
    "- ```~x``` means ```NOT x```. So you tell that you want the criteria ```x``` to be False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pwyifsFGkvbf"
   },
   "outputs": [],
   "source": [
    "df_after_2010_rating_equal_5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F-p2LX2CMcad"
   },
   "source": [
    "### Exercise\n",
    "\n",
    "1. Get the books that were published before 1850 or after 2008.\n",
    "2. Which book was published after 2005 and has a rating lower than 2.5?\n",
    "3. Calculate the mean and standard deviation of the year.\n",
    "4. Which formula is implemented in the method `std()` and `var()` of the pandas data frame?\n",
    "5. Calculate the median and the first and the third quartile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JwUlprgeY6Jt"
   },
   "outputs": [],
   "source": [
    "####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vaRU0z9kjpl3"
   },
   "source": [
    "## Cleaning: Examining the dataframe - a deeper look"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VJ-VPJN_jpl4"
   },
   "source": [
    "Beyond checking some quick general properties of the dataframe and looking at the first rows, we can dig a bit deeper into the values being stored. One thing that occurs frequently in real-world data are missing values. I have seen many forms of missing values, such as\n",
    "\n",
    "- -1 or 999 for the age of patients in hospital data\n",
    "- 0 for the height of patients\n",
    "- NaN (not a number)\n",
    "\n",
    "All of them need to be detected and taken care of because these missing values screw up the whole data analytics pipeline. Thereby, NaN is the nicest one because it is easy to spot.\n",
    "\n",
    "Let's start with this one and see for a column which seemed OK to us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get a sense of how many missing values there are in the dataframe.\n",
    "df['isbn'].isnull()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "1. Recall the function ```sum``` that you looked up above? Figure out how many values are missing in the column ```isbn```\n",
    "2. Combine the selection criteria with the function ```isnull()``` to print rows for which there is an ISBN\n",
    "3. See how many missing values every column has (you can also use a for-loop on ```df.columns``` here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PtIV9LmJe8Gl"
   },
   "outputs": [],
   "source": [
    "####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VRA_BiCGjpl-"
   },
   "source": [
    "## Cleaning: Dealing with Missing Values\n",
    "How should we interpret 'missing' or 'invalid' values in the data (hint: look at where these values occur)? One approach is to simply exclude them from the dataframe. Is this appropriate for all 'missing' or 'invalid' values? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "djHoEr3Ijpl_"
   },
   "outputs": [],
   "source": [
    "#Treat the missing or invalid values in the column 'year' of your dataframe\n",
    "df_clean = df[df['year'].notnull()].copy()\n",
    "\n",
    "print(df.shape)\n",
    "print(df_clean.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "stY0RLezjpmA"
   },
   "source": [
    "Ok so we have removed all the NaNs in the column ```year```. You can see that we only removed 7 rows, which is not too bad. Always check, how many rows you are removing as data is valuable. If you are removing 50% of the data, better think of another strategy than simply removing the rows.\n",
    "\n",
    "As you have probably noticed above, the data type of this column was ```float```. We can now try to change that to ```int``` which makes more sense for a year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_P0Uig8-jpmA"
   },
   "outputs": [],
   "source": [
    "df_clean['year'] = df_clean['year'].astype(int)\n",
    "print(df_clean.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QBXDF3x4jpmC"
   },
   "source": [
    "### Exercise\n",
    "\n",
    "1. Difficult: Let's try to change the data types of ```review_count```, ```isbn```, and ```rating_count``` to the appropriate data type int. Discuss which data type is appropriate. If the type conversion fails, we now know we have further problems and have to remove rows with missing values.\n",
    "\n",
    "Functions that can be helpful are:\n",
    "- df_clean['isbn'].str.isdigit() which returns a boolean vector of True where the entry of column isbn is a digit and False otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GjUv-iJQofut"
   },
   "outputs": [],
   "source": [
    "####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qXAS-kzvjpmE"
   },
   "source": [
    "Some of the other colums that should be strings have NaN. We now want to set them to \"\" --- an empty string. You might think about something like\n",
    "\n",
    "```\n",
    "df[df['genre_urls'].isnull()]['genre_urls'] = \"\"\n",
    "```\n",
    "\n",
    "Please try it out and see what happens.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lUiKZIDZ6184"
   },
   "source": [
    "As mentioned before, ```df[condition]``` is creating a view on the original dataset. And Pandas doesn't allow to change values on only a view to protect the user. Instead, we can use the ```loc``` function as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5WJ9YPU8jpmE"
   },
   "outputs": [],
   "source": [
    "df.loc[df['genre_urls'].isnull(), 'genre_urls']=\"\"\n",
    "df.loc[df['isbn'].isnull(), 'isbn']=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qpMjZXkA4aWS"
   },
   "outputs": [],
   "source": [
    "print(df['genre_urls'].isnull().sum())\n",
    "print(df['isbn'].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uLFsVFKg8NdU"
   },
   "source": [
    "Nice, now you learned the basic functionality of Pandas!!! Pandas is a super powerful package which I use daily. To learn how to work with data in Pandas Dataframes is extremely important and a skill that is very valuable. If you like you can continue learning Pandas using any kind of online course, e.g., https://www.datacamp.com/tutorial/pandas or https://www.youtube.com/watch?v=r-uOLxNrNk8."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
